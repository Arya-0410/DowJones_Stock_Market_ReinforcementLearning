{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on chunk 0 to 100\n",
      "Training on chunk 100 to 200\n",
      "Training on chunk 200 to 300\n",
      "Training on chunk 300 to 400\n",
      "Training on chunk 400 to 500\n",
      "Training on chunk 500 to 600\n",
      "Training on chunk 600 to 700\n",
      "Training on chunk 700 to 800\n",
      "Training on chunk 800 to 900\n",
      "Training on chunk 900 to 1000\n",
      "Training on chunk 1000 to 1100\n",
      "Training on chunk 1100 to 1200\n",
      "Training on chunk 1200 to 1300\n",
      "Training on chunk 1300 to 1400\n",
      "Training on chunk 1400 to 1500\n",
      "Training on chunk 1500 to 1600\n",
      "Training on chunk 1600 to 1700\n",
      "Training on chunk 1700 to 1800\n",
      "Training on chunk 1800 to 1900\n",
      "Training on chunk 1900 to 2000\n",
      "Training on chunk 2000 to 2100\n",
      "Best Actions with Positive Profit:\n",
      "      Timestamp Action  Profit\n",
      "3    1977-09-30    Buy    7.97\n",
      "7    1977-10-28    Buy   14.38\n",
      "9    1977-11-11    Buy   35.95\n",
      "11   1977-11-25    Buy    8.66\n",
      "14   1977-12-16    Buy    0.09\n",
      "...         ...    ...     ...\n",
      "2073 2017-07-14    Buy  223.40\n",
      "2075 2017-07-28    Buy  250.24\n",
      "2076 2017-08-04    Buy  262.50\n",
      "2079 2017-08-25    Buy  139.16\n",
      "2080 2017-08-29    Buy   51.70\n",
      "\n",
      "[1186 rows x 3 columns]\n",
      "Total Profit: 20993.059999999998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "\n",
    "# Define Model Architecture\n",
    "def build_model(state_size, action_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(16, activation='relu', input_dim=state_size),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(action_size, activation='linear')  # Q-values for actions\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Reward Function\n",
    "def calculate_reward(action, current_price, next_price):\n",
    "    if action == 0:  # Buy\n",
    "        return next_price - current_price\n",
    "    elif action == 1:  # Sell\n",
    "        return current_price - next_price\n",
    "    else:  # Hold\n",
    "        return 0\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    df['Price_diff'] = df['Value'].diff().fillna(0)\n",
    "    df['Moving_avg'] = df['Value'].rolling(window=10).mean().fillna(df['Value'].mean())\n",
    "    return df\n",
    "\n",
    "# Training RL Model on Data Chunks\n",
    "def train_model_on_chunks(data, model, state_size, action_size, chunk_size=100, episodes=10, batch_size=16):\n",
    "    memory = deque(maxlen=200)\n",
    "    gamma = 0.95  # Discount factor\n",
    "    epsilon = 1.0  # Exploration rate\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "\n",
    "    # Split data into chunks\n",
    "    for start_idx in range(0, len(data), chunk_size):\n",
    "        chunk_data = data.iloc[start_idx:start_idx + chunk_size]\n",
    "        print(f\"Training on chunk {start_idx} to {start_idx + chunk_size}\")\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = chunk_data[['Value', 'Price_diff', 'Moving_avg']].iloc[0].values\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "\n",
    "            for t in range(1, len(chunk_data) - 1):\n",
    "                # Choose action (exploration vs exploitation)\n",
    "                if np.random.rand() <= epsilon:\n",
    "                    action = random.randrange(action_size)\n",
    "                else:\n",
    "                    action = np.argmax(model.predict(state, verbose=0)[0])\n",
    "\n",
    "                # Get next state\n",
    "                next_state = chunk_data[['Value', 'Price_diff', 'Moving_avg']].iloc[t].values\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "                # Calculate reward\n",
    "                current_price = chunk_data.iloc[t - 1]['Value']\n",
    "                next_price = chunk_data.iloc[t]['Value']\n",
    "                reward = calculate_reward(action, current_price, next_price)\n",
    "\n",
    "                # Store in memory\n",
    "                memory.append((state, action, reward, next_state))\n",
    "\n",
    "                # Experience replay\n",
    "                if len(memory) > batch_size:\n",
    "                    minibatch = random.sample(memory, batch_size)\n",
    "                    states = np.array([m[0].flatten() for m in minibatch])\n",
    "                    next_states = np.array([m[3].flatten() for m in minibatch])\n",
    "                    next_q_values = model.predict(next_states, verbose=0)\n",
    "\n",
    "                    for i, (s, a, r, ns) in enumerate(minibatch):\n",
    "                        target = r + (gamma * np.amax(next_q_values[i]) if t != len(chunk_data) - 1 else r)\n",
    "                        target_f = model.predict(s, verbose=0)\n",
    "                        target_f[0][a] = target\n",
    "                        model.fit(s, target_f, epochs=1, verbose=0)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Decay exploration rate\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "\n",
    "    return model\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, data, state_size):\n",
    "    test_state = data[['Value', 'Price_diff', 'Moving_avg']].iloc[-1].values\n",
    "    test_state = np.reshape(test_state, [1, state_size])\n",
    "\n",
    "    actions_taken = []\n",
    "    profits = []\n",
    "    recommendations = []\n",
    "    timestamps = []\n",
    "\n",
    "    for t in range(len(data) - 1):\n",
    "        action = np.argmax(model.predict(test_state, verbose=0)[0])\n",
    "        actions_taken.append(action)\n",
    "\n",
    "        reward = calculate_reward(action, data.iloc[t]['Value'], data.iloc[t + 1]['Value'])\n",
    "        profits.append(reward)\n",
    "\n",
    "        timestamps.append(data.index[t + 1])\n",
    "        if action == 0:\n",
    "            recommendations.append('Buy')\n",
    "        elif action == 1:\n",
    "            recommendations.append('Sell')\n",
    "        else:\n",
    "            recommendations.append('Hold')\n",
    "\n",
    "    recommendation_df = pd.DataFrame({\n",
    "        'Timestamp': timestamps,\n",
    "        'Action': recommendations,\n",
    "        'Profit': profits\n",
    "    })\n",
    "\n",
    "    best_actions_df = recommendation_df[recommendation_df['Profit'] > 0]\n",
    "\n",
    "    print(\"Best Actions with Positive Profit:\")\n",
    "    print(best_actions_df)\n",
    "\n",
    "    total_profit = np.sum(profits)\n",
    "    print(f'Total Profit: {total_profit}')\n",
    "\n",
    "# Main Code\n",
    "# Load your data (assuming CSV with columns 'Time' and 'Value')\n",
    "data = pd.read_csv('DowJones.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Initialize model\n",
    "state_size = 3  # Value, Price_diff, Moving_avg\n",
    "action_size = 3  # Buy, Sell, Hold\n",
    "model = build_model(state_size, action_size)\n",
    "\n",
    "# Train model on data chunks\n",
    "chunk_size = 100\n",
    "model = train_model_on_chunks(data, model, state_size, action_size, chunk_size=chunk_size, episodes=10)\n",
    "\n",
    "# Evaluate the trained model\n",
    "evaluate_model(model, data, state_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
